# ğŸ§  IntelliSense AI â€“ Autonomous Agentic RAG

**IntelliSense AI** is a next-generation **Autonomous Agentic RAG** platform designed to provide self-optimizing, context-aware intelligence from your documents. Unlike simple RAG systems, IntelliSense AI uses a sophisticated multi-agent architecture (Controller, Orchestrator, Synthesizer) to understand user intent, self-correct retrieval failures, and deliver precise, high-confidence answers.

The system features an advanced **Retrieval Intelligence Engine** that learns from past interactions, adapts to query complexity, and aggressively filters noise to prevent hallucinations.

---

## ğŸ—ï¸ System Architecture

The system operates as a collaborative multi-agent pipeline:

```mermaid
flowchart TD
    User[User Query] --> Controller[Pipeline Controller]
    
    subgraph "Phase 1: Understanding"
        Controller --> Understand[Query Understanding Agent]
        Understand --> Intent[Intent & Scope Detection]
        Intent --> Rewrite[Structural Query Rewriting]
    end

    subgraph "Phase 2: Retrieval Intelligence"
        Rewrite --> Orchestrator[Retrieval Orchestrator]
        Orchestrator --> Search[Vector + Keyword + Section Search]
        Search --> Hierarchy[Hierarchical Rerank\n(Doc â†’ Section â†’ Chunk)]
        Hierarchy --> Clustering[Chunk Clustering & Dedup]
        Clustering --> Coverage{Coverage Check}
        Coverage -->|Gap Found| GapFill[Semantic Gap-Fill]
        Coverage -->|Sufficient| Confidence[Adaptive Confidence]
        GapFill --> Confidence
        Confidence --> Memory[Retrieval Memory\n(Learning Layer)]
    end

    subgraph "Phase 3: Synthesis & Verification"
        Memory --> Validation{Failure Prediction}
        Validation -->|High Risk| Grounded[Grounded Mode]
        Validation -->|Low Risk| Synth[Response Synthesizer]
        Grounded --> Synth
        Synth --> Verify[Context Verification]
    end

    Verify --> Final[Final Response]
```

---

## âœ¨ Retrieval Intelligence Engine

The core differentiator of IntelliSense AI is its **self-optimizing retrieval capabilities**:

### 1. ğŸ“‚ Hierarchical Retrieval (Structure-Aware)
Instead of flattening documents into isolated chunks, the system understands document structure. It prioritizes the best **Documents** first, then the most relevant **Sections** (e.g., "Methodology", "Conclusion"), and finally the specific **Chunks**. This ensures context is drawn from authoritative sections rather than random mentions.

### 2. ğŸ§  Long-Term Retrieval Memory
The system **learns** from every interaction. It tracks which retrieval patterns (e.g., "Definitions work best for conceptual queries") lead to successful answers. Over time, it builds a database of successful strategies and uses them to boost future retrieval performance.

### 3. ğŸ¯ Semantic Coverage Optimizer
The system explicitly extracts **key concepts** from your query and measures if the retrieved context covers them. If concepts are missing, it triggers targeted **Needle-in-a-Haystack** gap-fill queries to complete the picture before attempting to answer.

### 4. âš–ï¸ Adaptive Confidence Thresholds
Static thresholds fail because queries vary in difficulty. IntelliSense AI dynamically calculates confidence thresholds based on **Query Complexity** (simple vs. complex) and **Query Type** (Factual, Conceptual, Comparative). It demands stronger evidence for rigorous questions.

### 5. ğŸ”® Pre-Synthesis Failure Prediction
Before sending data to the LLM, a **Failure Predictor** analyzes the retrieved context. If it detects low coverage, fragmentation, or weak signals, it preemptively activates **Grounded Mode** or triggers a retry, preventing hallucinations before they happen.

### 6. ğŸ§© Semantic Chunk Clustering
To reduce noise, the system clusters semantically similar chunks and keeps only the highest-density representative. This removes redundancy and ensures the context window is filled with diverse, high-value information.

### 7. ğŸ” Structured Trace Logging
Every decisionâ€”from query expansion to failure predictionâ€”is logged in a structured trace. This provides complete observability into *why* the system retrieved specific content and how it made confidence decisions.

---

## ğŸ“‚ Project Structure

```text
IntelliSense-AI/
â”œâ”€â”€ app/
â”‚   â”œâ”€â”€ agents/
â”‚   â”‚   â”œâ”€â”€ pipeline_controller_agent/ # ğŸ§  The Brain (Flow Control)
â”‚   â”‚   â”œâ”€â”€ retrieval_agent/           # ğŸ” Orchestrator (Search & Optimization)
â”‚   â”‚   â”œâ”€â”€ query_understanding_agent/ # ğŸ—£ï¸ Intent & Scope Analysis
â”‚   â”‚   â”œâ”€â”€ response_synthesizer_agent/# âœï¸ Answer Generation
â”‚   â”‚   â”œâ”€â”€ claim_extraction_agent/    # ğŸ§ª EviLearn Fact Extraction
â”‚   â”‚   â””â”€â”€ verification_agent/        # âœ… EviLearn Fact Checking
â”‚   â”œâ”€â”€ rag/                           # âš™ï¸ Core Intelligence Modules
â”‚   â”‚   â”œâ”€â”€ hierarchical_retriever.py  # Structure-Aware Search
â”‚   â”‚   â”œâ”€â”€ retrieval_memory.py        # Learning Layer (SQLite)
â”‚   â”‚   â”œâ”€â”€ adaptive_confidence.py     # Dynamic Thresholds
â”‚   â”‚   â”œâ”€â”€ semantic_coverage.py       # Concept Gap-Filling
â”‚   â”‚   â”œâ”€â”€ failure_predictor.py       # Pre-Synthesis Guard
â”‚   â”‚   â”œâ”€â”€ chunk_clusterer.py         # Dedup & Clustering
â”‚   â”‚   â”œâ”€â”€ retrieval_trace.py         # Structured Logging
â”‚   â”‚   â””â”€â”€ retrieval_confidence.py    # Scoring Logic
â”‚   â”œâ”€â”€ api/                           # FastAPI Routes
â”‚   â”œâ”€â”€ core/                          # Config & Logging
â”‚   â””â”€â”€ storage/                       # Adapters (Local/S3/Pinecone)
â”œâ”€â”€ data/                              # Local Data Storage
â”œâ”€â”€ notebook-lm-frontend/              # âš›ï¸ React Frontend
â””â”€â”€ tests/                             # Unit & Smoke Tests
```

---

## ğŸš€ Quick Start (Local Mode)

All you need is Python, Node.js, and Docker.

### 1. Prerequisites
*   [**Python 3.10+**](https://www.python.org/)
*   [**Node.js (v18+)**](https://nodejs.org/)
*   [**Docker**](https://www.docker.com/) (for Redis)
*   **[uv](https://github.com/astral-sh/uv)** (Recommended package manager)

### 2. Start Infrastructure
Start Redis for session caching:
```bash
docker run -d --name redis-server -p 6379:6379 redis
```

### 3. Backend Setup
```bash
# Clone and enter repo
git clone https://github.com/chandankumar123456/intellisense-ai.git
cd intellisense-ai

# Install dependencies
uv sync

# Configure .env
cp .env.example .env
# Edit .env with your GROQ_API_KEY or OPENAI_API_KEY

# Run Server
uv run uvicorn app.main:app --reload
```
API Docs will be at: `http://localhost:8000/docs`

### 4. Frontend Setup
```bash
cd notebook-lm-frontend
npm install
npm start
```
Frontend will be at: `http://localhost:3000`

---

## âš™ï¸ Configuration

Key configuration flags in `app/core/config.py` allow you to toggle intelligence features:

```python
HIERARCHICAL_RETRIEVAL_ENABLED = True  # Enable/Disable structure awareness
RETRIEVAL_MEMORY_ENABLED = True        # Enable/Disable learning
ADAPTIVE_CONFIDENCE_ENABLED = True     # Enable/Disable dynamic thresholds
FAILURE_PREDICTION_ENABLED = True      # Enable/Disable pre-synthesis guards
CHUNK_CLUSTERING_ENABLED = True        # Enable/Disable redundancy removal
```

## ğŸ› ï¸ Troubleshooting

| Issue | Solution |
| :--- | :--- |
| **Redis Connection Error** | Check if Docker container `redis-server` is running. |
| **No Retrieval Results** | Ensure you have uploaded documents in the "Data" tab. Check `subject_filter` logs. |
| **Frontend Connection Refused** | Ensure backend is running on port 8000. |
| **Ingestion Errors** | Check `data/` directory permissions (Local Mode) or S3 credentials (AWS Mode). |

---

## ğŸ“œ License
MIT License
