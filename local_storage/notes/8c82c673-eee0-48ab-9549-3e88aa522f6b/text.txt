Abstract  
With  the  rapid  adoption  of  Large  Language  Models  (LLMs)  in  education,  students  increasingly  
rely
 
on
 
AI-generated
 
answers
 
without
 
clear
 
evidence
 
of
 
their
 
correctness
 
or
 
source
 
reliability.
 
This
 
raises
 
concerns
 
related
 
to
 
factual
 
accuracy,
 
explainability,
 
and
 
trust
 
in
 
AI-assisted
 
learning.
 
To
 
address
 
these
 
issues,
 
this
 
project
 
proposes
 
EviLearn
,
 
an
 
agentic
 
knowledge
 
reasoning
 
framework
 
that
 
combines
 
Retrieval-Augmented
 
Generation
 
(RAG)
 
with
 
explainable,
 
claim-level
 
verification
.
 
EviLearn  operates  by  decomposing  a  userâ€™s  answer  or  study  content  into  individual  factual  
claims
 
and
 
validating
 
each
 
claim
 
against
 
trusted
 
academic
 
documents
 
such
 
as
 
textbooks,
 
lecture
 
notes,
 
and
 
research
 
papers.
 
Using
 
an
 
agent-based
 
architecture,
 
the
 
system
 
performs
 
targeted
 
retrieval,
 
evaluates
 
evidence
 
relevance,
 
and
 
generates
 
confidence-backed
 
explanations
 
instead
 
of
 
producing
 
opaque
 
answers.
 
This
 
approach
 
improves
 
transparency,
 
reduces
 
hallucinations,
 
and
 
helps
 
learners
 
understand
 
why
 
an
 
answer
 
is
 
correct
 
or
 
incorrect.
 
Inspired  by  recent  research  in  Agentic  RAG ,  explainable  AI ,  and  educational  AI  systems ,  
EviLearn
 
emphasizes
 
source
 
traceability,
 
reasoning
 
transparency,
 
and
 
user
 
trust.
 
The
 
system
 
is
 
particularly
 
suitable
 
for
 
academic
 
environments
 
where
 
correctness
 
verification
 
and
 
explanation
 
quality
 
are
 
critical.
 
This
 
project
 
demonstrates
 
how
 
agentic
 
retrieval
 
and
 
explainable
 
reasoning
 
can
 
be
 
effectively
 
applied
 
to
 
support
 
reliable
 
and
 
evidence-driven
 
learning
 
systems.
 
 
